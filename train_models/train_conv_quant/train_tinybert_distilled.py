################################################
# æœ¬æ–‡ä»¶ç”¨äºè’¸é¦å¾®è°ƒTinyBertæ¨¡å‹ï¼ˆæœªè®¾ç½®éªŒè¯é›†ï¼‰
#
# åŠŸèƒ½è¯´æ˜ï¼š
# - è¾“å…¥: è®­ç»ƒé›†ï¼›æ•™å¸ˆæ¨¡å‹è·¯å¾„
# - è¾“å‡º: è’¸é¦å¾®è°ƒåçš„TinyBertæ¨¡å‹
#
# éœ€è¦ä¿®æ”¹çš„é…ç½®é¡¹ï¼ˆå…±4å¤„éœ€è¦ä¿®æ”¹ï¼Œå·²åœ¨ä»£ç ä¸­ç”¨æ³¨é‡Š"x"æ ‡è®°ï¼‰ï¼š
# 1. æ•™å¸ˆæ¨¡å‹ã€è®­ç»ƒé›†ã€æ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆ3å¤„éœ€è¦ä¿®æ”¹ï¼‰ï¼šç¬¬54-56è¡Œ
# 2. æ ‡ç­¾æ˜ å°„ï¼ˆ1å¤„éœ€è¦ä¿®æ”¹ï¼‰: ç¬¬59è¡Œ
################################################




import os
import torch
import numpy as np
import random
import csv
from tqdm.auto import tqdm
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    BertConfig,
    TrainingArguments,
    Trainer,
    PreTrainedTokenizer,
    PreTrainedModel
)
from torch.nn import KLDivLoss
from torch.utils.data import Dataset

# ====================== å›ºå®šéšæœºç§å­ ======================
SEED = 42
def set_seed(seed: int = SEED):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

set_seed()

# ====================== é…ç½® ======================
class Config:
    """é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®å‚æ•°"""
    TEACHER_MODEL_PATH = "trained_model_bert" # ä¿®æ”¹ä¸ºæ•™å¸ˆæ¨¡å‹è·¯å¾„ï¼ˆ1ï¼‰
    DATASET_PATH = "dataset.csv"        # ä¿®æ”¹ä¸ºè®­ç»ƒæ•°æ®è·¯å¾„ï¼ˆ2ï¼‰
    OUTPUT_DIR = "trained_model_tinybert_distilled"      # ä¿®æ”¹ä¸ºæ¨¡å‹è¾“å‡ºç›®å½•ï¼ˆ3ï¼‰
    MAX_LENGTH = 128                    # æœ€å¤§åºåˆ—é•¿åº¦
    
    # ä¿®æ”¹æ ‡ç­¾æ˜ å°„ï¼ˆ4ï¼‰
    LABEL_MAPPING = {
        0: "å…¶ä»–",
        1: "çˆ±å¥‡è‰º",
        2: "é£ä¹¦", 
        3: "é²å¤§å¸ˆ"
    }
    
    # å­¦ç”Ÿæ¨¡å‹é…ç½® (TinyBERTè§„æ ¼)
    STUDENT_CONFIG = {
        "hidden_size": 312,
        "num_hidden_layers": 4,
        "num_attention_heads": 12,
        "intermediate_size": 1200,
        "vocab_size": 30522,
        "num_labels": len(LABEL_MAPPING),
        "id2label": LABEL_MAPPING,
        "label2id": {v:k for k,v in LABEL_MAPPING.items()}
    }
    
    # è®­ç»ƒå‚æ•° (å¼ºåˆ¶ä½¿ç”¨FP32)
    TRAINING_ARGS = {
        "per_device_train_batch_size": 32,
        "num_train_epochs": 10, 
        "learning_rate": 5e-5,
        "weight_decay": 0.01,
        "warmup_steps": 500,
        "logging_steps": 100,
        "save_steps": 500,
        "output_dir": OUTPUT_DIR,
        "save_total_limit": 2,
        "disable_tqdm": False,
        "fp16": False,                  # ç¦ç”¨æ··åˆç²¾åº¦
        "bf16": False,                  # ç¦ç”¨bfloat16
        "tf32": False                   # ç¦ç”¨TF32
    }

# ====================== æ•°æ®é›†ç±» ======================
class DistillationDataset(Dataset):
    """ç”¨äºçŸ¥è¯†è’¸é¦çš„æ•°æ®é›†ç±»"""
    
    def __init__(self, 
                 tokenizer: PreTrainedTokenizer, 
                 file_path: str, 
                 max_length: int = Config.MAX_LENGTH):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.examples: List[Tuple[str, int]] = []
        self._load_data(file_path)
    
    def _load_data(self, file_path: str):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            reader = csv.reader(f)
            try:
                header = next(reader)
            except StopIteration:
                raise ValueError("CSVæ–‡ä»¶ä¸ºç©º")
            
            total_rows = sum(1 for _ in reader)
            f.seek(0)
            next(reader)
            
            valid_count = 0
            with tqdm(total=total_rows, desc="ğŸ“‚ åŠ è½½æ•°æ®é›†", unit="æ ·æœ¬") as pbar:
                for i, row in enumerate(reader, 1):
                    try:
                        if len(row) < 2:
                            raise ValueError("è¡Œç¼ºå°‘åˆ—")
                            
                        label = int(row[0].strip())
                        text = row[1].strip()
                        
                        if not text:
                            raise ValueError("æ–‡æœ¬ä¸ºç©º")
                        if label not in Config.LABEL_MAPPING:
                            raise ValueError(f"æ— æ•ˆæ ‡ç­¾ {label}")
                            
                        self.examples.append((text, label))
                        valid_count += 1
                    except Exception as e:
                        tqdm.write(f"âš ï¸ è·³è¿‡ç¬¬ {i} è¡Œ: {e} - å†…å®¹: {row}")
                    finally:
                        pbar.update(1)
        
        print(f"\nâœ… æˆåŠŸåŠ è½½ {valid_count}/{total_rows} ä¸ªæ ·æœ¬ (è·³è¿‡ {total_rows - valid_count} ä¸ªæ— æ•ˆè¡Œ)")
    
    def __len__(self) -> int:
        return len(self.examples)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        text, label = self.examples[idx]
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# ====================== è’¸é¦æŸå¤±å‡½æ•° ======================
class DistillationLoss:
    """çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°ï¼Œç»“åˆKLæ•£åº¦å’Œäº¤å‰ç†µ"""
    
    def __init__(self, temperature: float = 2.0, alpha: float = 0.7):
        self.temperature = temperature
        self.alpha = alpha
        self.kl_loss = KLDivLoss(reduction='batchmean')
        self.ce_loss = torch.nn.CrossEntropyLoss()
    
    def __call__(self, 
                 student_logits: torch.Tensor, 
                 teacher_logits: torch.Tensor, 
                 labels: torch.Tensor) -> torch.Tensor:
        soft_teacher = torch.nn.functional.softmax(teacher_logits / self.temperature, dim=-1)
        soft_student = torch.nn.functional.log_softmax(student_logits / self.temperature, dim=-1)
        kld_loss = self.kl_loss(soft_student, soft_teacher) * (self.temperature ** 2)
        ce_loss = self.ce_loss(student_logits, labels)
        return self.alpha * kld_loss + (1. - self.alpha) * ce_loss

# ====================== è’¸é¦Trainer ======================
class DistillationTrainer(Trainer):
    """è‡ªå®šä¹‰è’¸é¦Trainer"""
    
    def __init__(self, 
                 teacher_model: PreTrainedModel,
                 *args, 
                 **kwargs):
        super().__init__(*args, **kwargs)
        self.teacher_model = teacher_model.to(self.args.device)
        self.teacher_model.eval()
        self.distillation_loss = DistillationLoss()
        self.progress_bar = None
        self.start_time = datetime.now()
    
    def compute_loss(self, 
                    model: torch.nn.Module, 
                    inputs: Dict[str, torch.Tensor], 
                    return_outputs: bool = False) -> torch.Tensor:
        """è®¡ç®—è’¸é¦æŸå¤±"""
        outputs = model(**inputs)
        student_logits = outputs.logits
        
        with torch.no_grad():
            teacher_outputs = self.teacher_model(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask']
            )
            teacher_logits = teacher_outputs.logits
        
        loss = self.distillation_loss(
            student_logits,
            teacher_logits,
            inputs['labels']
        )
        
        return (loss, outputs) if return_outputs else loss
    
    def _inner_training_loop(self, *args, **kwargs):
        """è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯"""
        total_steps = self.args.max_steps if self.args.max_steps else \
            self.args.num_train_epochs * len(self.train_dataset) // self.args.per_device_train_batch_size
        
        self.progress_bar = tqdm(total=total_steps, desc="ğŸš€ è®­ç»ƒè¿›åº¦", unit="step")
        self.progress_bar.set_postfix({
            "epoch": "0",
            "loss": "0.0000",
            "lr": "0.0000",
            "elapsed": "0:00:00"
        })
        
        try:
            result = super()._inner_training_loop(*args, **kwargs)
        except Exception as e:
            self.progress_bar.close()
            raise e
        finally:
            if self.progress_bar is not None:
                self.progress_bar.close()
        
        return result
    
    def training_step(self, 
                     model: torch.nn.Module, 
                     inputs: Dict[str, torch.Tensor], 
                     num_items_in_batch: Optional[int] = None) -> torch.Tensor:
        """å•æ­¥è®­ç»ƒ"""
        model.train()
        inputs = self._prepare_inputs(inputs)
        
        with self.compute_loss_context_manager():
            loss = self.compute_loss(model, inputs)

        if self.args.gradient_accumulation_steps > 1 and num_items_in_batch is not None:
            loss = loss / num_items_in_batch

        loss.backward()

        if hasattr(self, 'progress_bar'):
            elapsed = datetime.now() - self.start_time
            current_lr = self.optimizer.param_groups[0]['lr'] if hasattr(self, 'optimizer') else 0
            self.progress_bar.set_postfix({
                "epoch": f"{self.state.epoch:.1f}",
                "loss": f"{loss.item():.4f}",
                "lr": f"{current_lr:.4e}",
                "elapsed": str(elapsed).split('.')[0]
            })
            self.progress_bar.update(1)

        return loss.detach()
    
    def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):
        """ä¿å­˜æ¨¡å‹"""
        if output_dir is None:
            output_dir = self.args.output_dir
        
        os.makedirs(output_dir, exist_ok=True)
        
        with tqdm(total=4, desc="ğŸ’¾ ä¿å­˜æ¨¡å‹", leave=False) as pbar:
            self.model.save_pretrained(output_dir)
            pbar.update(1)
            
            if self.tokenizer is not None:
                self.tokenizer.save_pretrained(output_dir)
            pbar.update(1)
            
            torch.save(self.state, os.path.join(output_dir, "trainer_state.pt"))
            pbar.update(1)
            
            with open(os.path.join(output_dir, "training_args.json"), "w") as f:
                f.write(str(self.args.to_dict()))
            pbar.update(1)

# ====================== ä¸»å‡½æ•° ======================
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("="*50)
    print(f"ğŸ§ª çŸ¥è¯†è’¸é¦å®éªŒ - {len(Config.LABEL_MAPPING)}åˆ†ç±»ä»»åŠ¡")
    print(f"ğŸ·ï¸ æ ‡ç­¾æ˜ å°„: {Config.LABEL_MAPPING}")
    print(f"ğŸ² éšæœºç§å­: {SEED}")
    print(f"ğŸ•’ å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*50 + "\n")
    
    # è®¾å¤‡æ£€æµ‹
    device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device.upper()}")
    
    # 1. åˆå§‹åŒ–ç»„ä»¶
    print("ğŸ”§ åˆå§‹åŒ–ç»„ä»¶...")
    try:
        tokenizer = BertTokenizer.from_pretrained(Config.TEACHER_MODEL_PATH)
        
        teacher_model = BertForSequenceClassification.from_pretrained(
            Config.TEACHER_MODEL_PATH,
            num_labels=len(Config.LABEL_MAPPING),
            id2label=Config.LABEL_MAPPING,
            label2id={v:k for k,v in Config.LABEL_MAPPING.items()}
        ).to(device)
        
        student_model = BertForSequenceClassification(
            BertConfig(**Config.STUDENT_CONFIG)
        ).to(device)
        
        print("âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    
    # 2. åŠ è½½æ•°æ®é›†
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    try:
        full_dataset = DistillationDataset(tokenizer, Config.DATASET_PATH)
        if len(full_dataset) == 0:
            raise ValueError("æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        raise
    
    # 3. è®­ç»ƒé…ç½®
    training_args = TrainingArguments(
        **Config.TRAINING_ARGS,
        eval_strategy="no",
        load_best_model_at_end=False,
        logging_dir=os.path.join(Config.OUTPUT_DIR, "logs"),
        report_to="none",
        no_cuda=(device != "cuda")  # éCUDAè®¾å¤‡ç¦ç”¨CUDA
    )
    
    # 4. å¼€å§‹è’¸é¦
    print("\n" + "="*50)
    print(f"ğŸ”¥ å¼€å§‹è’¸é¦ ({len(full_dataset)}ä¸ªæ ·æœ¬)")
    print(f"ğŸ‘¨â€ğŸ« æ•™å¸ˆæ¨¡å‹: {Config.TEACHER_MODEL_PATH}")
    print(f"ğŸ‘¨â€ğŸ“ å­¦ç”Ÿæ¨¡å‹æ¶æ„: hidden_size={Config.STUDENT_CONFIG['hidden_size']}, "
          f"num_layers={Config.STUDENT_CONFIG['num_hidden_layers']}")
    print(f"âš™ï¸ è®­ç»ƒå‚æ•°: epochs={Config.TRAINING_ARGS['num_train_epochs']}, "
          f"batch_size={Config.TRAINING_ARGS['per_device_train_batch_size']}, "
          f"lr={Config.TRAINING_ARGS['learning_rate']}")
    print(f"ğŸ”¢ ç²¾åº¦æ¨¡å¼: FP32")
    print("="*50 + "\n")
    
    try:
        trainer = DistillationTrainer(
            teacher_model=teacher_model,
            model=student_model,
            args=training_args,
            train_dataset=full_dataset,
            tokenizer=tokenizer
        )
        
        # è®­ç»ƒå‰æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
        teacher_params = sum(p.numel() for p in teacher_model.parameters())
        student_params = sum(p.numel() for p in student_model.parameters())
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°: æ•™å¸ˆæ¨¡å‹={teacher_params:,} | å­¦ç”Ÿæ¨¡å‹={student_params:,} "
              f"(å‹ç¼©ç‡: {teacher_params/student_params:.1f}x)")
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise
    
    # 5. ä¿å­˜æ¨¡å‹
    print("\n" + "="*50)
    print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {Config.OUTPUT_DIR}")
    
    try:
        # ç›´æ¥ä¿å­˜åˆ°ä¸»è¾“å‡ºç›®å½•ï¼ˆä¸bertå’Œtinybertè®­ç»ƒä¿æŒä¸€è‡´ï¼‰
        trainer.save_model(Config.OUTPUT_DIR)
        tokenizer.save_pretrained(Config.OUTPUT_DIR)
        
        # ä¿å­˜æ ‡ç­¾æ˜ å°„
        import json
        with open(os.path.join(Config.OUTPUT_DIR, "label_mapping.json"), "w") as f:
            json.dump({
                "id2label": Config.LABEL_MAPPING,
                "label2id": {v:k for k,v in Config.LABEL_MAPPING.items()}
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… è’¸é¦å®Œæˆ! æ¨¡å‹å·²ä¿å­˜è‡³: {Config.OUTPUT_DIR}")
        print(f"æ€»è®­ç»ƒæ—¶é—´: {str(datetime.now() - trainer.start_time).split('.')[0]}")
        print(f"æ ‡ç­¾æ˜ å°„: {Config.LABEL_MAPPING}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()
